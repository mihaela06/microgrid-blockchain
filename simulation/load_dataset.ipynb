{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create appliances individual consumption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download files from figshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilename(cd):\n",
    "    \"\"\"\n",
    "    Get filename from content-disposition\n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if len(fname) == 0:\n",
    "        return None\n",
    "    return fname[0].strip('\"')\n",
    "\n",
    "\n",
    "def downloadFile(url):\n",
    "    \"\"\"\n",
    "    Download file from URL\n",
    "    Returns file name\n",
    "    \"\"\"\n",
    "    headers = {\"Range\": \"bytes=0-100\"}\n",
    "    r = requests.get(url, allow_redirects=True, headers=headers)\n",
    "    filename = getFilename(r.headers.get('content-disposition'))\n",
    "    if not os.path.exists(filename):\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(filename, 'wb').write(r.content)\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://figshare.com/ndownloader/files/26179625\",  # house_00\n",
    "    \"https://figshare.com/ndownloader/files/26179622\",  # house_01\n",
    "    \"https://figshare.com/ndownloader/files/26179628\",  # house_02\n",
    "    \"https://figshare.com/ndownloader/files/26179640\",  # house_03\n",
    "    \"https://figshare.com/ndownloader/files/26179643\",  # house_04\n",
    "    \"https://figshare.com/ndownloader/files/26179646\",  # house_05\n",
    "    \"https://figshare.com/ndownloader/files/26179649\",  # house_06\n",
    "    \"https://figshare.com/ndownloader/files/26179658\",  # house_07\n",
    "    \"https://figshare.com/ndownloader/files/26179694\",  # house_08\n",
    "    \"https://figshare.com/ndownloader/files/26179697\",  # house_09\n",
    "    \"https://figshare.com/ndownloader/files/26179700\",  # house_10\n",
    "    \"https://figshare.com/ndownloader/files/26179703\",  # house_11\n",
    "    \"https://figshare.com/ndownloader/files/26179706\",  # house_12\n",
    "    \"https://figshare.com/ndownloader/files/26179709\",  # house_13\n",
    "    \"https://figshare.com/ndownloader/files/26179712\"   # house_14\n",
    "]\n",
    "\n",
    "archives = []\n",
    "\n",
    "for u in urls:\n",
    "    archives.append(downloadFile(u))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse appliances data in Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data and keep only appliances with annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = []\n",
    "\n",
    "for archive in archives:\n",
    "    with zipfile.ZipFile(archive, \"r\") as zip:\n",
    "        zip.extractall()\n",
    "\n",
    "        folder = archive.split('.')[0]\n",
    "\n",
    "        items = pd.read_csv(os.path.join(folder, \"items.tsv\"), sep=\"\\t\")\n",
    "        items_no = 0\n",
    "        for index, item in items.iterrows():\n",
    "            labels = pd.read_csv(os.path.join(\n",
    "                folder, \"item_{:04d}_annotation_labels.tsv\".format(item.item_id)), sep=\"\\t\")\n",
    "            if len(labels) > 0:\n",
    "                items_no += 1\n",
    "            else:\n",
    "                os.remove(os.path.join(\n",
    "                    folder, \"item_{:04d}_data.tsv.gz\".format(item.item_id)))\n",
    "                os.remove(os.path.join(\n",
    "                    folder, \"item_{:04d}_annotation_labels.tsv\".format(item.item_id)))\n",
    "                os.remove(os.path.join(\n",
    "                    folder, \"item_{:04d}_annotations.tsv\".format(item.item_id)))\n",
    "        if items_no > 0:\n",
    "            folders.append(folder)\n",
    "        else:\n",
    "            for root, dirs, files in os.walk(folder, topdown=False):\n",
    "                for name in files:\n",
    "                    os.remove(os.path.join(root, name))\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "            os.rmdir(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDir(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCES_FOLDER = \"appliances\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeDir(APPLIANCES_FOLDER)\n",
    "\n",
    "for folder in folders:\n",
    "    items = pd.read_csv(os.path.join(folder, \"items.tsv\"), sep=\"\\t\")\n",
    "    for index, item in items.iterrows():\n",
    "        try:\n",
    "            labels = pd.read_csv(os.path.join(\n",
    "                folder, \"item_{:04d}_annotation_labels.tsv\".format(item.item_id)), sep=\"\\t\")\n",
    "            annotations = pd.read_csv(os.path.join(\n",
    "                folder, \"item_{:04d}_annotations.tsv\".format(item.item_id)), sep=\"\\t\", infer_datetime_format=True)\n",
    "            annotations.start_date = pd.to_datetime(\n",
    "                annotations.start_date, format='%Y-%m-%d %H:%M:%S')\n",
    "            annotations.stop_date = pd.to_datetime(\n",
    "                annotations.stop_date, format='%Y-%m-%d %H:%M:%S')\n",
    "            data = pd.read_csv(os.path.join(\n",
    "                folder, \"item_{:04d}_data.tsv.gz\".format(item.item_id)), sep=\"\\t\", compression=\"gzip\")\n",
    "            data.time = pd.to_datetime(data.time, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            makeDir(os.path.join(APPLIANCES_FOLDER, item.category))\n",
    "            makeDir(os.path.join(os.path.join(APPLIANCES_FOLDER, item.category),\n",
    "                                 item.label\n",
    "                                 ))\n",
    "\n",
    "            for _, label in labels.iterrows():\n",
    "                makeDir(os.path.join(\n",
    "                        os.path.join(\n",
    "                            os.path.join(APPLIANCES_FOLDER, item.category),\n",
    "                            item.label),\n",
    "                        label.text\n",
    "                        ))\n",
    "\n",
    "            for _, label in labels.iterrows():\n",
    "                counter = 0\n",
    "                for index, annotation in annotations.iterrows():\n",
    "                    if annotation.label_id == label.id:\n",
    "                        df = data.loc[(data['time'] >= annotation.start_date)\n",
    "                                      & (data['time'] < annotation.stop_date)]\n",
    "                        df.to_csv(os.path.join(\n",
    "                            os.path.join(\n",
    "                                os.path.join(\n",
    "                                    os.path.join(\n",
    "                                        APPLIANCES_FOLDER, item.category),\n",
    "                                    item.label),\n",
    "                                label.text\n",
    "                            ),\n",
    "                            str(counter) + \".csv\"\n",
    "                        ))\n",
    "                        counter += 1\n",
    "        except Exception as e:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove erroneous data and unlabelled programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, _ in os.walk(APPLIANCES_FOLDER, topdown=False):\n",
    "    for dir in dirs:\n",
    "        for new_root, programs, files in os.walk(os.path.join(APPLIANCES_FOLDER, dir), topdown=False):\n",
    "            try:\n",
    "                lens = []\n",
    "                for file in files:\n",
    "                    df = pd.read_csv(os.path.join(new_root, file))\n",
    "                    lens.append(len(df))\n",
    "                if len(lens) == 0:\n",
    "                    continue\n",
    "                len_avg = np.average(lens)\n",
    "                for file in files:\n",
    "                    df = pd.read_csv(os.path.join(new_root, file))\n",
    "                    # maximum variation admitted - 20% from the average program length to remove outliers\n",
    "                    if np.abs(len_avg - len(df)) > 0.2 * len_avg:\n",
    "                        os.remove(os.path.join(new_root, file))\n",
    "                if len(os.listdir(new_root)) == 0:\n",
    "                    os.rmdir(new_root)\n",
    "            except:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, _ in os.walk(APPLIANCES_FOLDER, topdown=False):\n",
    "    for dir in dirs:\n",
    "        if dir == \"Error\" or dir == \"Unknown\" or dir == \"Unkown\":\n",
    "            for root, dirs, files in os.walk(os.path.join(root, dir), topdown=False):\n",
    "                for name in files:\n",
    "                    os.remove(os.path.join(root, name))\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "            os.rmdir(os.path.join(root))\n",
    "        if dir == \"unknown activity\":\n",
    "            os.rename(os.path.join(root, dir), os.path.join(root, \"Idle\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove archives and unuseful extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\".\", topdown=False):\n",
    "    for file in files:\n",
    "        if file[:5] == \"house\":\n",
    "            os.remove(os.path.join(root, file))\n",
    "    for dir in dirs:\n",
    "        if dir[:5] == \"house\":\n",
    "            for root, dirs, files in os.walk(dir, topdown=False):\n",
    "                for name in files:\n",
    "                    os.remove(os.path.join(root, name))\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "                os.rmdir(dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim unuseful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(APPLIANCES_FOLDER, topdown=False):\n",
    "    for file in files:\n",
    "        if file[-4:] == \".csv\":\n",
    "            path = os.path.join(root, file)\n",
    "            df = pd.read_csv(path)\n",
    "            if len(df.columns) == 4:\n",
    "                df[\"value\"].to_csv(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create consumer background consumption dataset (from non-smart appliances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download files from figshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://figshare.com/ndownloader/files/28120062\",  # 201903\n",
    "    \"https://figshare.com/ndownloader/files/28120176\",  # 201904\n",
    "    \"https://figshare.com/ndownloader/files/28120242\",  # 201905\n",
    "    \"https://figshare.com/ndownloader/files/28130424\",  # 202003\n",
    "    \"https://figshare.com/ndownloader/files/28128384\",  # 202004\n",
    "    \"https://figshare.com/ndownloader/files/28130430\",  # 202005    \n",
    "]\n",
    "\n",
    "archives = []\n",
    "\n",
    "for u in urls:\n",
    "    archives.append(downloadFile(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKGROUND_FOLDER = \"background\"\n",
    "makeDir(BACKGROUND_FOLDER)\n",
    "\n",
    "ids_complete = []\n",
    "\n",
    "for archive in archives:\n",
    "    df = pd.read_csv(archive, compression=\"gzip\")\n",
    "    df.dropna(inplace=True)\n",
    "    id_counts = df['id'].value_counts()\n",
    "    ids_to_keep = id_counts[id_counts > id_counts.max() - 5].index\n",
    "    df = df.loc[df['id'].isin(ids_to_keep)]\n",
    "    ids_complete.append(df.id.unique())\n",
    "    del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ids = list(set.intersection(*map(set, ids_complete)))\n",
    "common_ids = np.array(common_ids, dtype=np.int64)\n",
    "common_ids.sort()\n",
    "print(common_ids)\n",
    "print(len(common_ids))\n",
    "first20 = common_ids[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for archive in archives:\n",
    "    year = archive.split(\"_\")[2][:4]\n",
    "    year_path = os.path.join(BACKGROUND_FOLDER, year)\n",
    "    makeDir(year_path)\n",
    "    month = archive.split(\"_\")[2][4:6]\n",
    "    month_path = os.path.join(year_path, month)\n",
    "    makeDir(month_path)\n",
    "    df = pd.read_csv(archive, compression=\"gzip\")\n",
    "    df.rename(columns={df.columns[0]: 'timestamp'}, inplace=True)\n",
    "    for index in range(len(first20)):\n",
    "        df_filtered = df.loc[df['id'] == first20[index]]\n",
    "        df_filtered.to_csv(os.path.join(month_path, str(index) + \".csv\"))\n",
    "    del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [\"2019\", \"2020\"]\n",
    "MONTHS = [\"03\", \"04\", \"05\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_del = set()\n",
    "\n",
    "\n",
    "for year in YEARS:\n",
    "    year_path = os.path.join(BACKGROUND_FOLDER, year)\n",
    "    for month in MONTHS:\n",
    "        month_path = os.path.join(year_path, month)\n",
    "        for index in range(20):\n",
    "            df = pd.read_csv(os.path.join(month_path, str(index) + \".csv\"))\n",
    "            if df['value'].sum() < 50:\n",
    "                to_be_del.add(index)\n",
    "            else:\n",
    "                df['value'] *= 1000\n",
    "                df[['timestamp', 'value']].to_csv(os.path.join(month_path, str(index) + \".csv\"))\n",
    "\n",
    "for id in to_be_del:\n",
    "    for year in YEARS:\n",
    "        year_path = os.path.join(BACKGROUND_FOLDER, year)\n",
    "        for month in MONTHS:\n",
    "            month_path = os.path.join(year_path, month)\n",
    "            os.remove(os.path.join(month_path, str(id) + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\".\", topdown=False):\n",
    "    for file in files:\n",
    "        if file[:11] == \"consumption\":\n",
    "            os.remove(os.path.join(root, file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create energy production dataset from weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_FOLDER = \"weather\"\n",
    "\n",
    "makeDir(WEATHER_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read API key from file smart-meter/dataset/.apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".apikey\", \"rt\") as f:\n",
    "    API_KEY = f.readline().strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temporary email using 1secmail API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_response = requests.get(\"https://www.1secmail.com/api/v1/?action=genRandomMailbox&count=1\")\n",
    "mail = json.loads(address_response.content)[0]\n",
    "user = mail.split(\"@\")[0]\n",
    "domain = mail.split(\"@\")[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create request for data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS:\n",
    "    url = \"https://developer.nrel.gov/api/nsrdb/v2/solar/msg-iodc-download.csv?api_key=\" + API_KEY + \\\n",
    "        \"&full_name=Mihaela+Mihaiu&email=mihaela.mihaiu@mta.ro&affiliation=MTA&reason=Academic&mailing_list=false&wkt=POINT(44.57+26.06)&names=\" + year + \\\n",
    "        \"&attributes=ghi,air_temperature,surface_pressure,relative_humidity,wind_speed&leap_day=false&utc=false&interval=15\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(os.path.join(WEATHER_FOLDER, year + \"_raw.csv\"), \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    time.sleep(2)   # 2 seconds between API requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS:\n",
    "    df = pd.read_csv(os.path.join(\n",
    "        WEATHER_FOLDER, year + \"_raw.csv\"), low_memory=False, skiprows=2)\n",
    "    df['timestamp'] = pd.to_datetime(\n",
    "        df[[\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]])\n",
    "\n",
    "    Rd = 287.058    # specific gas constant for dry air\n",
    "    Rv = 461.495    # specific gas constant for water vapor\n",
    "\n",
    "    df['p1'] = 6.1078 * pow(10, (7.5 * df['Temperature']) /\n",
    "                            (df['Temperature'] + 273.3))\n",
    "    df['pv'] = df['p1'] * df['Relative Humidity']\n",
    "    df['pd'] = df['Pressure'] * 100 - df['pv']\n",
    "    df['air_density'] = (df['pd'] / (Rd * (df['Temperature'] + 273.3))) + \\\n",
    "        (df['pv'] / (Rv * (df['Temperature'] + 273.3)))   # kg/m3\n",
    "    df[[\"timestamp\", \"GHI\", \"air_density\", \"Wind Speed\"]].to_csv(os.path.join(\n",
    "        WEATHER_FOLDER, year + \".csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS:\n",
    "    os.remove(os.path.join(\n",
    "        WEATHER_FOLDER, year + \"_raw.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify generators models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://photovoltaic-software.com/principle-ressources/how-calculate-solar-energy-power-pv-systems\n",
    "\n",
    "SOLAR_PANELS_FOLDER = \"Solar Panel\"\n",
    "\n",
    "solar_panels = {\n",
    "    # https://www.europe-solarstore.com/download/lg/LG_Neon_R_Prime_LG355-370Q1K-V5_data_sheet.pdf\n",
    "    \"LG370Q1K-V5\": {\n",
    "        \"area\": 1.568,\n",
    "        \"efficiency\": 21.4,\n",
    "        \"pr\": 0.78\n",
    "    },\n",
    "    # https://www.europe-solarstore.com/download/sunpower/SunPower-SPR-MAX3-390_Datasheet_EN.pdf\n",
    "    \"SunPower SPR-MAX3-400\": {\n",
    "        \"area\": 1.659,\n",
    "        \"efficiency\": 22.6,\n",
    "        \"pr\": 0.75\n",
    "    },\n",
    "    # https://www.europe-solarstore.com/download/benq/BenQ_Sunforte_PM096B00_datasheet.pdf\n",
    "    \"BenQ SunForte PM096B00-335\": {\n",
    "        \"area\": 1.631,\n",
    "        \"efficiency\": 19.6,\n",
    "        \"pr\": 0.72\n",
    "    }\n",
    "}\n",
    "\n",
    "# Power = A × r × H × PR\n",
    "\n",
    "WIND_TURBINES_FOLDER = \"Wind Turbine\"\n",
    "\n",
    "wind_turbines = {\n",
    "    \"AIR 40\": {\n",
    "        \"swept_area\": 1.07,\n",
    "        \"efficiency\": 0.45\n",
    "    },\n",
    "    \"Pikasola B08F4SYCF7\": {\n",
    "        \"swept_area\": 1.14,\n",
    "        \"efficiency\": 0.40\n",
    "    },\n",
    "    \"Raptor G4\": {\n",
    "        \"swept_area\": 1.70,\n",
    "        \"efficiency\": 0.43\n",
    "    }\n",
    "}\n",
    "\n",
    "# Power = 0.5 x Swept Area x Air Density x Velocity3 * efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_path = os.path.join(APPLIANCES_FOLDER, SOLAR_PANELS_FOLDER)\n",
    "\n",
    "makeDir(solar_path)\n",
    "\n",
    "for panel in solar_panels:\n",
    "    panel_path = os.path.join(solar_path, panel)\n",
    "    makeDir(panel_path)\n",
    "\n",
    "wind_path = os.path.join(APPLIANCES_FOLDER, WIND_TURBINES_FOLDER)\n",
    "\n",
    "makeDir(wind_path)\n",
    "\n",
    "for turbine in wind_turbines:\n",
    "    turbine_path = os.path.join(wind_path, turbine)\n",
    "    makeDir(turbine_path)\n",
    "\n",
    "for year in YEARS:\n",
    "    df = pd.read_csv(os.path.join(WEATHER_FOLDER, year + \".csv\"))\n",
    "    for month in MONTHS:\n",
    "        df['timestamp'] = pd.to_datetime(df.timestamp)\n",
    "        dt = datetime.datetime(year=int(year), month=int(month), day=1)\n",
    "        start = pd.to_datetime(calendar.timegm(dt.timetuple()), unit='s')\n",
    "        dt = datetime.datetime(year=int(year), month=int(month)+1, day=1)\n",
    "        stop = pd.to_datetime(calendar.timegm(dt.timetuple()), unit='s')\n",
    "        df_month = df.loc[(df['timestamp'] >= start)\n",
    "                          & (df['timestamp'] < stop)]\n",
    "        days = np.unique(df_month[\"timestamp\"].dt.date)\n",
    "        for index in range(len(days) - 1):\n",
    "            df_day = df_month.loc[(df_month['timestamp'] >=\n",
    "                                   pd.to_datetime(days[index])) & (df_month['timestamp'] < pd.to_datetime(days[index+1]))]\n",
    "\n",
    "            for panel in solar_panels:\n",
    "                panel_path = os.path.join(solar_path, panel)\n",
    "                panel_path = os.path.join(\n",
    "                    panel_path, year+month+f\"{index+1:02d}\"+\".csv\")\n",
    "                data = solar_panels[panel]\n",
    "                values = data[\"area\"] * data[\"efficiency\"] * \\\n",
    "                    data[\"pr\"] * df_day[\"GHI\"]\n",
    "                df_panel = pd.DataFrame(\n",
    "                    data={'timestamp': df_day['timestamp'], 'value': values})\n",
    "                df_panel.to_csv(panel_path)\n",
    "\n",
    "            for turbine in wind_turbines:\n",
    "                turbine_path = os.path.join(wind_path, turbine)\n",
    "                turbine_path = os.path.join(\n",
    "                    turbine_path, year+month+f\"{index+1:02d}\"+\".csv\")\n",
    "                data = wind_turbines[turbine]\n",
    "                values = 0.5 * data[\"swept_area\"] * data[\"efficiency\"] * \\\n",
    "                    df_day[\"air_density\"] * pow(df_day[\"Wind Speed\"], 3)\n",
    "                df_turbine = pd.DataFrame(\n",
    "                    data={'timestamp': df_day['timestamp'], 'value': values})\n",
    "                df_turbine.to_csv(turbine_path)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
